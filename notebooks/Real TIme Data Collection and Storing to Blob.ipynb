{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d1ecd3-72a1-4be9-b061-d328db9ddabb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347a5e39-76c3-4867-a39c-01fa2e6bd82a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Real-Time Data Collection:\n",
    "Set up a pipeline to continuously collect real-time stock prices and other relevant financial data.\n",
    "Use an API like yfinance, Alpha Vantage, or a direct market data provider to fetch real-time data.\n",
    "Store this data in your Azure Blob Storage or a database for easy access and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11d8d31-8d16-4eca-ba99-ec7157d5d292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting yfinance\n  Downloading yfinance-0.2.40-py2.py3-none-any.whl (73 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.5/73.5 kB 1.3 MB/s eta 0:00:00\nRequirement already satisfied: azure-storage-blob in /databricks/python3/lib/python3.10/site-packages (12.19.0)\nRequirement already satisfied: sqlalchemy in /databricks/python3/lib/python3.10/site-packages (1.4.39)\nRequirement already satisfied: pyodbc in /databricks/python3/lib/python3.10/site-packages (4.0.32)\nRequirement already satisfied: numpy>=1.16.5 in /databricks/python3/lib/python3.10/site-packages (from yfinance) (1.23.5)\nCollecting multitasking>=0.0.7\n  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\nRequirement already satisfied: pytz>=2022.5 in /databricks/python3/lib/python3.10/site-packages (from yfinance) (2022.7)\nRequirement already satisfied: lxml>=4.9.1 in /databricks/python3/lib/python3.10/site-packages (from yfinance) (4.9.1)\nRequirement already satisfied: platformdirs>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from yfinance) (2.5.2)\nCollecting peewee>=3.16.2\n  Downloading peewee-3.17.5.tar.gz (3.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 37.3 MB/s eta 0:00:00\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nCollecting frozendict>=2.3.4\n  Downloading frozendict-2.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.3/117.3 kB 11.7 MB/s eta 0:00:00\nCollecting requests>=2.31\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 7.8 MB/s eta 0:00:00\nCollecting html5lib>=1.1\n  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.2/112.2 kB 8.3 MB/s eta 0:00:00\nRequirement already satisfied: pandas>=1.3.0 in /databricks/python3/lib/python3.10/site-packages (from yfinance) (1.5.3)\nRequirement already satisfied: beautifulsoup4>=4.11.1 in /databricks/python3/lib/python3.10/site-packages (from yfinance) (4.11.1)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob) (0.6.1)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob) (39.0.1)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob) (1.29.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob) (4.4.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy) (2.0.1)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.2.post1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.31->yfinance) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2.0.4)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\nBuilding wheels for collected packages: peewee\n  Building wheel for peewee (pyproject.toml): started\n  Building wheel for peewee (pyproject.toml): finished with status 'done'\n  Created wheel for peewee: filename=peewee-3.17.5-cp310-cp310-linux_x86_64.whl size=727736 sha256=26a8f2b96997ca427653591b8d97a8c3d34b320857e6f50dc00e5ee0b03dc177\n  Stored in directory: /root/.cache/pip/wheels/e2/53/cd/01e683140f777fc841bcb4ced360a727a4ea3109fb4fc69c0d\nSuccessfully built peewee\nInstalling collected packages: peewee, multitasking, requests, html5lib, frozendict, yfinance\n  Attempting uninstall: requests\n    Found existing installation: requests 2.28.1\n    Not uninstalling requests at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-33d118a2-7d9b-4c23-80a3-c19227c7d457\n    Can't uninstall 'requests'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-engineering 0.2.1 requires pyspark<4,>=3.1.2, which is not installed.\ndatabricks-sdk 0.1.6 requires requests<2.29.0,>=2.28.1, but you have requests 2.32.3 which is incompatible.\nSuccessfully installed frozendict-2.4.4 html5lib-1.1 multitasking-0.0.11 peewee-3.17.5 requests-2.32.3 yfinance-0.2.40\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install yfinance azure-storage-blob sqlalchemy pyodbc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45600282-45bd-42c0-b9bb-4e69b5bd4197",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fetch Realtime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17c5f65-8ce5-457a-a00e-0b4b9f3670db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BANKBARODA.NS  HDFCBANK.NS  ...        ^BSESN         ^NSEI\n0     266.950012       1737.5  ...  80316.351562  24345.449219\n\n[1 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_real_time_stock_price(stock_symbol):\n",
    "    stock = yf.Ticker(stock_symbol)\n",
    "    hist = stock.history(period='1d')\n",
    "    if hist.empty:  # Check if the DataFrame is empty\n",
    "        print(f\"No data found for {stock_symbol}\")\n",
    "        return None  # Return None or appropriate value indicating no data\n",
    "    else:\n",
    "        return hist['Close'].iloc[-1]\n",
    "\n",
    "def fetch_data():\n",
    "    # List of stock symbols to fetch\n",
    "    stocks = ['BANKBARODA.NS', 'HDFCBANK.NS', 'SBIN.NS', 'ICICIBANK.NS', 'AXISBANK.NS', '^BSESN', '^NSEI']\n",
    "    data = {}\n",
    "    for stock in stocks:\n",
    "        price = get_real_time_stock_price(stock)\n",
    "        if price is not None:  # Only add to data if price is not None\n",
    "            data[stock] = price\n",
    "    return data\n",
    "\n",
    "# Fetch and display the data\n",
    "data = fetch_data()\n",
    "df = pd.DataFrame(data, index=[0])\n",
    "print(df)  # Use print if display is not available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d9af7a-73e5-41a7-9555-e260518deac6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## upload to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d030de-c286-43e3-a8ea-93c7ba7638a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to blob storage as realtime_data_20240704_045907.csv\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "\n",
    "# Azure Storage connection string\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=storageriskpredictor;AccountKey=VFB3FzSHo02JqmvdOaq2Ygr2MR5Tdq+3N/O6yTeRvr2HVysRrDK8BsmTW2u4Smp7rOBZWWD/McRO+AStGLAQzQ==;EndpointSuffix=core.windows.net'\n",
    "container_name = 'riskpredict-data'\n",
    "\n",
    "# Initialize the BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "def upload_blob(dataframe, file_name):\n",
    "    # Convert DataFrame to CSV\n",
    "    csv_data = dataframe.to_csv(index=False)\n",
    "\n",
    "    # Create a BlobClient\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "\n",
    "    # Upload the CSV data\n",
    "    blob_client.upload_blob(csv_data, overwrite=True)\n",
    "\n",
    "# Generate a file name based on current timestamp and upload the DataFrame\n",
    "file_name = f\"realtime_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "upload_blob(df, file_name)\n",
    "print(f\"Data uploaded to blob storage as {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff1bf56-48e2-413d-8e59-5506ac47cf12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "TIll here  I was fetching real time data and storing it in a seperate csv file. But I now want to upload the realtime data in the merged model. an train my model for better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7a26d8-2e15-43d4-85c0-8056f3af8acd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. List and Read the Real-Time Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5a5247-398e-4a50-90d6-f61994275377",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from io import StringIO\n",
    "\n",
    "# Azure Blob Storage configuration\n",
    "account_name = 'storageriskpredictor'\n",
    "account_key = 'VFB3FzSHo02JqmvdOaq2Ygr2MR5Tdq+3N/O6yTeRvr2HVysRrDK8BsmTW2u4Smp7rOBZWWD/McRO+AStGLAQzQ=='\n",
    "container_name = 'riskpredict-data'\n",
    "\n",
    "# Initialize Blob Service Client\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key)\n",
    "\n",
    "# Function to list all real-time data files\n",
    "def list_blob_files(container_name):\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_list = container_client.list_blobs()\n",
    "    return [blob.name for blob in blob_list if \"realtime_data_\" in blob.name]\n",
    "\n",
    "# List all real-time data files\n",
    "real_time_data_files = list_blob_files(container_name)\n",
    "\n",
    "# Load the real-time data into DataFrames\n",
    "real_time_data = []\n",
    "for file in real_time_data_files:\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=file)\n",
    "    # Read the content of the blob as a string\n",
    "    data_str = blob_client.download_blob().content_as_text()\n",
    "    # Convert the string to a DataFrame\n",
    "    real_time_data.append(pd.read_csv(StringIO(data_str)))\n",
    "\n",
    "# Concatenate real-time data into a single DataFrame\n",
    "real_time_data_df = pd.concat(real_time_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe2f1b9-e436-40ae-87d9-7af20bbe87b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed Real-Time Data Columns:\nIndex(['Close_BANKBARODA', 'Close_HDFCBANK', 'Close_SBIN', 'Close_ICICIBANK',\n       'Close_AXISBANK', 'Close_BSE_SENSEX', 'Close_NSEI'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "real_time_data_df.rename(columns={\n",
    "    'BANKBARODA.NS': 'Close_BANKBARODA',\n",
    "    'HDFCBANK.NS': 'Close_HDFCBANK',\n",
    "    'SBIN.NS': 'Close_SBIN',\n",
    "    'ICICIBANK.NS': 'Close_ICICIBANK',\n",
    "    'AXISBANK.NS': 'Close_AXISBANK',\n",
    "    '^BSESN': 'Close_BSE_SENSEX',\n",
    "    '^NSEI': 'Close_NSEI'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Renamed Real-Time Data Columns:\")\n",
    "print(real_time_data_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d198368-eb2f-4a6e-a969-c9b0edbcd1de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Merge Real-Time Data with Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08455d2f-8fa8-46e6-bb47-5792c74ffb22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to the historical dataset in Azure Blob Storage\n",
    "historical_data_path = \"/dbfs/mnt/riskpredict-data/merged_dataset.csv\"\n",
    "\n",
    "# Load the historical data\n",
    "historical_data = pd.read_csv(historical_data_path)\n",
    "\n",
    "# Ensure columns match and concatenate historical and real-time data\n",
    "updated_data = pd.concat([historical_data, real_time_data_df], ignore_index=True)\n",
    "\n",
    "# Drop duplicates and sort by date if necessary\n",
    "updated_data.drop_duplicates(inplace=True)\n",
    "updated_data.sort_values(by='Date', inplace=True)  # Replace 'date_column' with the actual date column name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed463bd4-5af3-4bd7-960b-7fb9b5a94faf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Data Preview:\n                  Date   Open  ...  Close_BSE_SENSEX    Close_NSEI\n0  2019-01-02 15:30:00  283.0  ...          35891.52  23465.599609\n1  2019-01-03 15:30:00  283.0  ...          35513.71  23465.599609\n2  2019-01-04 15:30:00  283.0  ...          35695.10  23465.599609\n3  2019-01-07 15:30:00  283.0  ...          35850.16  23465.599609\n4  2019-01-08 15:30:00  283.0  ...          35980.93  23465.599609\n\n[5 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate historical and real-time data\n",
    "updated_data = pd.concat([historical_data, real_time_data_df], ignore_index=True)\n",
    "\n",
    "# Drop duplicates and sort by date if necessary\n",
    "updated_data.drop_duplicates(inplace=True)\n",
    "updated_data.sort_values(by='Date', inplace=True)  # Ensure 'Date' column is appropriately handled\n",
    "\n",
    "print(\"Updated Data Preview:\")\n",
    "print(updated_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf683119-8b8d-4b6a-9ba3-1823300da46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) preview after filling missing values:\n     Open   High         Low  ...  Close_AXISBANK  Close_BSE_SENSEX    Close_NSEI\n0  283.0  287.5  281.600006  ...     1181.050049          35891.52  23465.599609\n1  283.0  287.5  281.600006  ...     1181.050049          35513.71  23465.599609\n2  283.0  287.5  281.600006  ...     1181.050049          35695.10  23465.599609\n3  283.0  287.5  281.600006  ...     1181.050049          35850.16  23465.599609\n4  283.0  287.5  281.600006  ...     1181.050049          35980.93  23465.599609\n\n[5 rows x 11 columns]\nTarget (y) preview:\n 0    286.25\n1    286.25\n2    286.25\n3    286.25\n4    286.25\nName: Close_BANKBARODA, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = updated_data.drop(['Date', 'Close_BANKBARODA'], axis=1)\n",
    "y = updated_data['Close_BANKBARODA']\n",
    "\n",
    "# Fill missing values with the mean of each column\n",
    "X_filled = X.fillna(X.mean())\n",
    "\n",
    "print(\"Features (X) preview after filling missing values:\\n\", X_filled.head())\n",
    "print(\"Target (y) preview:\\n\", y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcb3e430-5f05-4b9d-a644-6cfdf214a9ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Train the Model with the Updated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d9ffe1-6bbd-4b4e-ba40-034614e0eabe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to save the updated dataset\n",
    "updated_data_path = \"/dbfs/mnt/riskpredict-data/updated_merged_dataset.csv\"\n",
    "\n",
    "# Save the updated data to DBFS\n",
    "updated_data.to_csv(updated_data_path, index=False)\n",
    "\n",
    "# Load the updated dataset for model training\n",
    "model_data = pd.read_csv(updated_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77aaa01-878c-400b-9255-9fde973d3338",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Prepare X and y:\n",
    "### Now, ensuring X includes all the feature columns and y is the target variable (e.g., Close_BANKBARODA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3202f7-4d9f-44fd-9be9-8382757210c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) preview after filling missing values:\n     Open   High         Low  ...  Close_AXISBANK  Close_BSE_SENSEX    Close_NSEI\n0  283.0  287.5  281.600006  ...     1181.050049          35891.52  23465.599609\n1  283.0  287.5  281.600006  ...     1181.050049          35513.71  23465.599609\n2  283.0  287.5  281.600006  ...     1181.050049          35695.10  23465.599609\n3  283.0  287.5  281.600006  ...     1181.050049          35850.16  23465.599609\n4  283.0  287.5  281.600006  ...     1181.050049          35980.93  23465.599609\n\n[5 rows x 11 columns]\nTarget (y) preview:\n 0    286.25\n1    286.25\n2    286.25\n3    286.25\n4    286.25\nName: Close_BANKBARODA, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = updated_data.drop(['Date', 'Close_BANKBARODA'], axis=1)\n",
    "y = updated_data['Close_BANKBARODA']\n",
    "\n",
    "# Fill missing values with the mean of each column\n",
    "X_filled = X.fillna(X.mean())\n",
    "\n",
    "print(\"Features (X) preview after filling missing values:\\n\", X_filled.head())\n",
    "print(\"Target (y) preview:\\n\", y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f2cf17-e3f3-4f09-a6b4-4bc21024433b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1080, 11)\nX_test shape: (270, 11)\ny_train shape: (1080,)\ny_test shape: (270,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d720397c-0981-492e-a29d-867c13255d1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6f44031d674155954ae7247ff0ab4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f772daed-e88a-4d48-a2e2-3530dca7cd61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0\nR^2 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3b6cc50-0a94-4f92-86fc-05b23556c2d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Update: 04-07-2024\n",
    "\n",
    "### Automating Real-Time Data Integration and Model Training\n",
    "\n",
    "Today, we automated the integration of real-time data and updated our Random Forest model training process. Below are the detailed steps completed:\n",
    "\n",
    "1. **Real-Time Data Integration:**\n",
    "   - Automated fetching of real-time data files from Azure Blob Storage.\n",
    "   - Merged real-time data with historical data after aligning column names.\n",
    "   - Handled missing values by filling them with the mean of each column.\n",
    "\n",
    "2. **Model Training with Updated Dataset:**\n",
    "   - Prepared features (X) and target variable (y) from the updated dataset.\n",
    "   - Split the dataset into training and testing sets.\n",
    "   - Trained a Random Forest model with the training data.\n",
    "   - Evaluated the model using Mean Squared Error (MSE) and R^2 Score.\n",
    "\n",
    "#### Model Evaluation Results:\n",
    "\n",
    "- **Mean Squared Error:** 0.0\n",
    "- **R^2 Score:** 1.0\n",
    "\n",
    "These perfect evaluation metrics suggest that the model may be overfitting, meaning it performs exceptionally well on the training data but might not generalize well to unseen data. This will be addressed in the next steps.\n",
    "\n",
    "3. **Job Scheduling:**\n",
    "   - Scheduled a job to automate the daily process of fetching real-time data, merging it with historical data, and updating the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1c9c09-619d-4ca0-9a02-863a450cf172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Mean Squared Error: 1.2447514756948308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_filled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert negative MSE scores to positive\n",
    "cv_mse_scores = -cv_scores\n",
    "\n",
    "print(f\"Cross-Validation Mean Squared Error: {cv_mse_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63a23c7f-be80-4079-98da-4202558be41e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b0fb91-6b30-472d-bb52-c70cb86828a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb57bd712a847c69ca4861d69c64dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21addd1c56cc4f3cbc64b246dfe61685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a974d9d8cc472c9f3866edbca3cac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=50)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_filled, y)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model = RandomForestRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Real TIme Data Collection and Storing to Blob",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
